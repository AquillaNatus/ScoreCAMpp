{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":998277,"sourceType":"datasetVersion","datasetId":547506},{"sourceId":6229028,"sourceType":"datasetVersion","datasetId":3577960}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.listdir('/kaggle/input/val-mini/val_mini')","metadata":{"id":"fdk_Fkl3itXj","outputId":"90ac90ee-a5d3-4a55-d73e-77fbe718c872","execution":{"iopub.status.busy":"2024-01-07T19:02:10.583124Z","iopub.execute_input":"2024-01-07T19:02:10.583511Z","iopub.status.idle":"2024-01-07T19:02:10.590647Z","shell.execute_reply.started":"2024-01-07T19:02:10.583480Z","shell.execute_reply":"2024-01-07T19:02:10.589456Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"['n01440764', 'n01494475', 'n01484850', 'n01491361', 'n01443537']"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nUTILS\n\"\"\"\n#!/usr/bin/env python\n\"\"\"flashtorch.utils\n\nThis module provides utility functions for image handling and tensor\ntransformation.\n\n\"\"\"\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as F\nimport torch.nn.functional as ff\nimport cv2\nimport torch\n\ndef load_image(image_path):\n    \"\"\"Loads image as a PIL RGB image.\n\n        Args:\n            - **image_path (str) - **: A path to the image\n\n        Returns:\n            An instance of PIL.Image.Image in RGB\n\n    \"\"\"\n\n    return Image.open(image_path).convert('RGB')\n\n\ndef apply_transforms(image, size=224):\n    \"\"\"Transforms a PIL image to torch.Tensor.\n\n    Applies a series of tranformations on PIL image including a conversion\n    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n    is ready to be used as an input to neural networks.\n\n    First the image is resized to 256, then cropped to 224. The `means` and\n    `stds` for normalisation are taken from numbers used in ImageNet, as\n    currently developing the package for visualizing pre-trained models.\n\n    The plan is to to expand this to handle custom size/mean/std.\n\n    Args:\n        image (PIL.Image.Image or numpy array)\n        size (int, optional, default=224): Desired size (width/height) of the\n            output tensor\n\n    Shape:\n        Input: :math:`(C, H, W)` for numpy array\n        Output: :math:`(N, C, H, W)`\n\n    Returns:\n        torch.Tensor (torch.float32): Transformed image tensor\n\n    Note:\n        Symbols used to describe dimensions:\n            - N: number of images in a batch\n            - C: number of channels\n            - H: height of the image\n            - W: width of the image\n\n    \"\"\"\n\n    if not isinstance(image, Image.Image):\n        image = F.to_pil_image(image)\n\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n\n    transform = transforms.Compose([\n        transforms.Resize(size),\n        transforms.CenterCrop(size),\n        transforms.ToTensor(),\n        transforms.Normalize(means, stds)\n    ])\n\n    tensor = transform(image).unsqueeze(0)\n\n    tensor.requires_grad = True\n\n    return tensor\n\ndef apply_transforms_v0(image, size=224):\n    \"\"\"Transforms a PIL image to torch.Tensor.\n\n    Applies a series of tranformations on PIL image including a conversion\n    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n    is ready to be used as an input to neural networks.\n\n    First the image is resized to 256, then cropped to 224. The `means` and\n    `stds` for normalisation are taken from numbers used in ImageNet, as\n    currently developing the package for visualizing pre-trained models.\n\n    The plan is to to expand this to handle custom size/mean/std.\n\n    Args:\n        image (PIL.Image.Image or numpy array)\n        size (int, optional, default=224): Desired size (width/height) of the\n            output tensor\n\n    Shape:\n        Input: :math:`(C, H, W)` for numpy array\n        Output: :math:`(N, C, H, W)`\n\n    Returns:\n        torch.Tensor (torch.float32): Transformed image tensor\n\n    Note:\n        Symbols used to describe dimensions:\n            - N: number of images in a batch\n            - C: number of channels\n            - H: height of the image\n            - W: width of the image\n\n    \"\"\"\n\n    if not isinstance(image, Image.Image):\n        image = F.to_pil_image(image)\n\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n\n    transform = transforms.Compose([\n        transforms.Resize(size),\n        transforms.CenterCrop(size),\n        transforms.ToTensor()\n    ])\n\n    tensor = transform(image).unsqueeze(0)\n\n    tensor.requires_grad = True\n\n    return tensor\n\n\ndef denormalize(tensor):\n    \"\"\"Reverses the normalisation on a tensor.\n\n    Performs a reverse operation on a tensor, so the pixel value range is\n    between 0 and 1. Useful for when plotting a tensor into an image.\n\n    Normalisation: (image - mean) / std\n    Denormalisation: image * std + mean\n\n    Args:\n        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n\n    Shape:\n        Input: :math:`(N, C, H, W)`\n        Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Return:\n        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n            values between [0, 1]\n\n    Note:\n        Symbols used to describe dimensions:\n            - N: number of images in a batch\n            - C: number of channels\n            - H: height of the image\n            - W: width of the image\n\n    \"\"\"\n\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n\n    denormalized = tensor.clone()\n\n    for channel, mean, std in zip(denormalized[0], means, stds):\n        channel = channel.mul(std).add(mean)\n\n\n    return denormalized\n\n\ndef standardize_and_clip(tensor, min_value=0.0, max_value=1.0):\n    \"\"\"Standardizes and clips input tensor.\n\n    Standardize the input tensor (mean = 0.0, std = 1.0), ensures std is 0.1\n    and clips it to values between min/max (default: 0.0/1.0).\n\n    Args:\n        tensor (torch.Tensor):\n        min_value (float, optional, default=0.0)\n        max_value (float, optional, default=1.0)\n\n    Shape:\n        Input: :math:`(C, H, W)`\n        Output: Same as the input\n\n    Return:\n        torch.Tensor (torch.float32): Normalised tensor with values between\n            [min_value, max_value]\n\n    \"\"\"\n\n    tensor = tensor.detach().cpu()\n\n    mean = tensor.mean()\n    std = tensor.std()\n    if std == 0:\n        std += 1e-7\n\n    standardized = tensor.sub(mean).div(std).mul(0.1)\n    clipped = standardized.add(0.5).clamp(min_value, max_value)\n\n    return clipped\n\n\ndef format_for_plotting(tensor):\n    \"\"\"Formats the shape of tensor for plotting.\n\n    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n    which is not suitable for plotting as images. This function formats an\n    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n    data.\n\n    Args:\n        tensor (torch.Tensor, torch.float32): Image tensor\n\n    Shape:\n        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n\n    Return:\n        torch.Tensor (torch.float32): Formatted image tensor (detached)\n\n    Note:\n        Symbols used to describe dimensions:\n            - N: number of images in a batch\n            - C: number of channels\n            - H: height of the image\n            - W: width of the image\n\n    \"\"\"\n\n    has_batch_dimension = len(tensor.shape) == 4\n    formatted = tensor.clone()\n\n    if has_batch_dimension:\n        formatted = tensor.squeeze(0)\n\n    if formatted.shape[0] == 1:\n        return formatted.squeeze(0).detach()\n    else:\n        return formatted.permute(1, 2, 0).detach()\n\n\ndef visualize(input_, gradients, save_path=None, cmap='viridis', alpha=0.7):\n\n    \"\"\" Method to plot the explanation.\n\n        # Arguments\n            input_: Tensor. Original image.\n            gradients: Tensor. Saliency map result.\n            save_path: String. Defaults to None.\n            cmap: Defaults to be 'viridis'.\n            alpha: Defaults to be 0.7.\n\n    \"\"\"\n\n    input_ = format_for_plotting(denormalize(input_))\n    gradients = format_for_plotting(standardize_and_clip(gradients))\n\n    subplots = [\n        ('Input image', [(input_, None, None)]),\n        ('Saliency map across RGB channels', [(gradients, None, None)]),\n        ('Overlay', [(input_, None, None), (gradients, cmap, alpha)])\n    ]\n\n    num_subplots = len(subplots)\n\n    fig = plt.figure(figsize=(16, 3))\n\n    for i, (title, images) in enumerate(subplots):\n        ax = fig.add_subplot(1, num_subplots, i + 1)\n        ax.set_axis_off()\n\n        for image, cmap, alpha in images:\n            ax.imshow(image, cmap=cmap, alpha=alpha)\n\n        ax.set_title(title)\n    if save_path is not None:\n        plt.savefig(save_path)\n\n\ndef basic_visualize(input_, gradients, save_path=None, weight=None, cmap='viridis', alpha=0.7):\n\n    \"\"\" Method to plot the explanation.\n\n        # Arguments\n            input_: Tensor. Original image.\n            gradients: Tensor. Saliency map result.\n            save_path: String. Defaults to None.\n            cmap: Defaults to be 'viridis'.\n            alpha: Defaults to be 0.7.\n\n    \"\"\"\n    input_ = format_for_plotting(denormalize(input_))\n    gradients = format_for_plotting(standardize_and_clip(gradients))\n\n    subplots = [\n        ('Saliency map across RGB channels', [(gradients, None, None)]),\n        ('Overlay', [(input_, None, None), (gradients, cmap, alpha)])\n    ]\n\n    num_subplots = len(subplots)\n\n    fig = plt.figure(figsize=(4, 4))\n\n    for i, (title, images) in enumerate(subplots):\n        ax = fig.add_subplot(1, num_subplots, i + 1)\n        ax.set_axis_off()\n\n        for image, cmap, alpha in images:\n            # ax.imshow(image, cmap=cmap, alpha=alpha)\n            ax.imshow(image,alpha=alpha)\n    if save_path is not None:\n        plt.savefig(save_path)\n\n\ndef find_resnet_layer(arch, target_layer_name):\n    \"\"\"Find resnet layer to calculate GradCAM and GradCAM++\n\n    Args:\n        arch: default torchvision densenet models\n        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n            target_layer_name = 'conv1'\n            target_layer_name = 'layer1'\n            target_layer_name = 'layer1_basicblock0'\n            target_layer_name = 'layer1_basicblock0_relu'\n            target_layer_name = 'layer1_bottleneck0'\n            target_layer_name = 'layer1_bottleneck0_conv1'\n            target_layer_name = 'layer1_bottleneck0_downsample'\n            target_layer_name = 'layer1_bottleneck0_downsample_0'\n            target_layer_name = 'avgpool'\n            target_layer_name = 'fc'\n\n    Return:\n        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n    if target_layer_name is None:\n        target_layer_name = 'layer4'\n\n    if 'layer' in target_layer_name:\n        hierarchy = target_layer_name.split('_')\n        layer_num = int(hierarchy[0].lstrip('layer'))\n        if layer_num == 1:\n            target_layer = arch.layer1\n        elif layer_num == 2:\n            target_layer = arch.layer2\n        elif layer_num == 3:\n            target_layer = arch.layer3\n        elif layer_num == 4:\n            target_layer = arch.layer4\n        else:\n            raise ValueError('unknown layer : {}'.format(target_layer_name))\n\n        if len(hierarchy) >= 2:\n            bottleneck_num = int(hierarchy[1].lower().lstrip('bottleneck').lstrip('basicblock'))\n            target_layer = target_layer[bottleneck_num]\n\n        if len(hierarchy) >= 3:\n            target_layer = target_layer._modules[hierarchy[2]]\n\n        if len(hierarchy) == 4:\n            target_layer = target_layer._modules[hierarchy[3]]\n\n    else:\n        target_layer = arch._modules[target_layer_name]\n\n    return target_layer\n\n\ndef find_densenet_layer(arch, target_layer_name):\n    \"\"\"Find densenet layer to calculate GradCAM and GradCAM++\n\n    Args:\n        arch: default torchvision densenet models\n        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n            target_layer_name = 'features'\n            target_layer_name = 'features_transition1'\n            target_layer_name = 'features_transition1_norm'\n            target_layer_name = 'features_denseblock2_denselayer12'\n            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n            target_layer_name = 'features_denseblock2_denselayer12_norm1'\n            target_layer_name = 'classifier'\n\n    Return:\n        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n\n    if target_layer_name is None:\n        target_layer_name = 'features'\n\n    hierarchy = target_layer_name.split('_')\n    target_layer = arch._modules[hierarchy[0]]\n\n    if len(hierarchy) >= 2:\n        target_layer = target_layer._modules[hierarchy[1]]\n\n    if len(hierarchy) >= 3:\n        target_layer = target_layer._modules[hierarchy[2]]\n\n    if len(hierarchy) == 4:\n        target_layer = target_layer._modules[hierarchy[3]]\n\n    return target_layer\n\n\ndef find_vgg_layer(arch, target_layer_name):\n    \"\"\"Find vgg layer to calculate GradCAM and GradCAM++\n\n    Args:\n        arch: default torchvision densenet models\n        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n            target_layer_name = 'features'\n            target_layer_name = 'features_42'\n            target_layer_name = 'classifier'\n            target_layer_name = 'classifier_0'\n\n    Return:\n        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n    if target_layer_name is None:\n        target_layer_name = 'features'\n\n    hierarchy = target_layer_name.split('_')\n\n    if len(hierarchy) >= 1:\n        target_layer = arch.features\n\n    if len(hierarchy) == 2:\n        target_layer = target_layer[int(hierarchy[1])]\n    # print(f'The target layer is: {target_layer}')\n    return target_layer\n\n\ndef find_alexnet_layer(arch, target_layer_name):\n    \"\"\"Find alexnet layer to calculate GradCAM and GradCAM++\n\n    Args:\n        arch: default torchvision densenet models\n        target_layer_name (str): the name of layer with its hierarchical information. please refer to usages below.\n            target_layer_name = 'features'\n            target_layer_name = 'features_0'\n            target_layer_name = 'classifier'\n            target_layer_name = 'classifier_0'\n\n    Return:\n        target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n    if target_layer_name is None:\n        target_layer_name = 'features_29'\n\n    hierarchy = target_layer_name.split('_')\n\n    if len(hierarchy) >= 1:\n        target_layer = arch.features\n\n    if len(hierarchy) == 2:\n        target_layer = target_layer[int(hierarchy[1])]\n\n    return target_layer\n\n\ndef find_squeezenet_layer(arch, target_layer_name):\n    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n\n        Args:\n            - **arch - **: default torchvision densenet models\n            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n                target_layer_name = 'features_12'\n                target_layer_name = 'features_12_expand3x3'\n                target_layer_name = 'features_12_expand3x3_activation'\n\n        Return:\n            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n    if target_layer_name is None:\n        target_layer_name = 'features'\n\n    hierarchy = target_layer_name.split('_')\n    target_layer = arch._modules[hierarchy[0]]\n\n    if len(hierarchy) >= 2:\n        target_layer = target_layer._modules[hierarchy[1]]\n\n    if len(hierarchy) == 3:\n        target_layer = target_layer._modules[hierarchy[2]]\n\n    elif len(hierarchy) == 4:\n        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n\n    return target_layer\n\n\ndef find_googlenet_layer(arch, target_layer_name):\n    \"\"\"Find squeezenet layer to calculate GradCAM and GradCAM++\n\n        Args:\n            - **arch - **: default torchvision googlenet models\n            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n                target_layer_name = 'inception5b'\n\n        Return:\n            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n    if target_layer_name is None:\n        target_layer_name = 'features'\n\n    hierarchy = target_layer_name.split('_')\n    target_layer = arch._modules[hierarchy[0]]\n\n    if len(hierarchy) >= 2:\n        target_layer = target_layer._modules[hierarchy[1]]\n\n    if len(hierarchy) == 3:\n        target_layer = target_layer._modules[hierarchy[2]]\n\n    elif len(hierarchy) == 4:\n        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n\n    return target_layer\n\n\ndef find_mobilenet_layer(arch, target_layer_name):\n    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n\n        Args:\n            - **arch - **: default torchvision googlenet models\n            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n                target_layer_name = 'features'\n\n        Return:\n            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n    if target_layer_name is None:\n        target_layer_name = 'features'\n\n    hierarchy = target_layer_name.split('_')\n    target_layer = arch._modules[hierarchy[0]]\n\n    if len(hierarchy) >= 2:\n        target_layer = target_layer._modules[hierarchy[1]]\n\n    if len(hierarchy) == 3:\n        target_layer = target_layer._modules[hierarchy[2]]\n\n    elif len(hierarchy) == 4:\n        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n\n    return target_layer\n\n\ndef find_shufflenet_layer(arch, target_layer_name):\n    \"\"\"Find mobilenet layer to calculate GradCAM and GradCAM++\n\n        Args:\n            - **arch - **: default torchvision googlenet models\n            - **target_layer_name (str) - **: the name of layer with its hierarchical information. please refer to usages below.\n                target_layer_name = 'conv5'\n\n        Return:\n            target_layer: found layer. this layer will be hooked to get forward/backward pass information.\n    \"\"\"\n    if target_layer_name is None:\n        target_layer_name = 'features'\n\n    hierarchy = target_layer_name.split('_')\n    target_layer = arch._modules[hierarchy[0]]\n\n    if len(hierarchy) >= 2:\n        target_layer = target_layer._modules[hierarchy[1]]\n\n    if len(hierarchy) == 3:\n        target_layer = target_layer._modules[hierarchy[2]]\n\n    elif len(hierarchy) == 4:\n        target_layer = target_layer._modules[hierarchy[2] + '_' + hierarchy[3]]\n\n    return target_layer\n\n\ndef find_layer(arch, target_layer_name):\n    \"\"\"Find target layer to calculate CAM.\n\n        : Args:\n            - **arch - **: Self-defined architecture.\n            - **target_layer_name - ** (str): Name of target class.\n\n        : Return:\n            - **target_layer - **: Found layer. This layer will be hooked to get forward/backward pass information.\n    \"\"\"\n\n    if target_layer_name.split('_') not in arch._modules.keys():\n        raise Exception(\"Invalid target layer name.\")\n    target_layer = arch._modules[target_layer_name]\n    return target_layer\n\n\n\n\ndef visualize(img, cam):\n    \"\"\"\n    Synthesize an image with CAM to make a result image.\n    Args:\n        img: (Tensor) shape => (1, 3, H, W)\n        cam: (Tensor) shape => (1, 1, H', W')\n    Return:\n        synthesized image (Tensor): shape =>(1, 3, H, W)\n    \"\"\"\n\n    _, _, H, W = img.shape\n    img = img.detach().cpu()\n    cam = cam.detach().cpu()\n    cam = ff.interpolate(cam, size=(H, W), mode='bilinear', align_corners=False)\n    cam = 255 * cam.squeeze()\n    heatmap = cv2.applyColorMap(np.uint8(cam), cv2.COLORMAP_JET)\n    heatmap = torch.from_numpy(heatmap.transpose(2, 0, 1))\n    heatmap = heatmap.float() / 255\n    b, g, r = heatmap.split(1)\n    heatmap = torch.cat([r, g, b])\n\n    result = heatmap + img.cpu()\n    result = result.div(result.max())\n    result = torch.squeeze(result)\n    # print(f'Shape of result is: {result.shape}')\n    result = result.permute(1,2,0)\n    plt.imshow(result)\n    plt.show()\n    plt.imsave('tower_red.png', result)\n    return result","metadata":{"id":"eoQi1gjGiozj","execution":{"iopub.status.busy":"2024-01-07T19:02:10.725004Z","iopub.execute_input":"2024-01-07T19:02:10.725402Z","iopub.status.idle":"2024-01-07T19:02:10.786402Z","shell.execute_reply.started":"2024-01-07T19:02:10.725354Z","shell.execute_reply":"2024-01-07T19:02:10.785396Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# **ScoreCAM**","metadata":{}},{"cell_type":"code","source":"'''\nPart of code borrows from https://github.com/1Konny/gradcam_plus_plus-pytorch\n'''\n\nimport torch\nimport torch.nn.functional as f\n\nclass BaseCAM(object):\n    \"\"\" Base class for Class activation mapping.\n\n        : Args\n            - **model_dict -** : Dict. Has format as dict(type='vgg', arch=torchvision.models.vgg16(pretrained=True),\n            layer_name='features',input_size=(224, 224)).\n\n    \"\"\"\n\n    def __init__(self, model_dict):\n        model_type = model_dict['type']\n        layer_name = model_dict['layer_name']\n\n        self.model_arch = model_dict['arch']\n        self.model_arch.eval()\n        if torch.cuda.is_available():\n          self.model_arch.cuda()\n        self.gradients = dict()\n        self.activations = dict()\n\n        def backward_hook(module, grad_input, grad_output):\n            if torch.cuda.is_available():\n              self.gradients['value'] = grad_output[0].cuda()\n            else:\n              self.gradients['value'] = grad_output[0]\n            return None\n\n        def forward_hook(module, input, output):\n            if torch.cuda.is_available():\n              self.activations['value'] = output.cuda()\n            else:\n              self.activations['value'] = output\n            return None\n\n        if 'vgg' in model_type.lower():\n            self.target_layer = find_vgg_layer(self.model_arch, layer_name)\n        elif 'resnet' in model_type.lower():\n            self.target_layer = find_resnet_layer(self.model_arch, layer_name)\n        elif 'densenet' in model_type.lower():\n            self.target_layer = find_densenet_layer(self.model_arch, layer_name)\n        elif 'alexnet' in model_type.lower():\n            self.target_layer = find_alexnet_layer(self.model_arch, layer_name)\n        elif 'squeezenet' in model_type.lower():\n            self.target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n        elif 'googlenet' in model_type.lower():\n            self.target_layer = find_googlenet_layer(self.model_arch, layer_name)\n        elif 'shufflenet' in model_type.lower():\n            self.target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n        elif 'mobilenet' in model_type.lower():\n            self.target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n        else:\n            self.target_layer = find_layer(self.model_arch, layer_name)\n\n        self.target_layer.register_forward_hook(forward_hook)\n        self.target_layer.register_backward_hook(backward_hook)\n\n    def forward(self, input, class_idx=None, retain_graph=False):\n        return None\n\n    def __call__(self, input, class_idx=None, retain_graph=False):\n        return self.forward(input, class_idx, retain_graph)\n\n\n# ScoreCAM\n\nclass ScoreCAM(BaseCAM):\n\n    \"\"\"\n        ScoreCAM, inherit from BaseCAM\n\n    \"\"\"\n\n    def __init__(self, model_dict):\n        super().__init__(model_dict)\n        self.predicted_confidence_cam = 0\n        self.predicted_confidence_cam_list = []\n        self.avg_drop = 0\n        self.avg_increase = 0\n    def forward(self, input, class_idx=None, retain_graph=False):\n        b, c, h, w = input.size()\n\n        # predication on raw input\n        logit = self.model_arch(input).cuda()\n\n        if class_idx is None:\n            predicted_class = logit.max(1)[-1]\n            score = logit[:, logit.max(1)[-1]].squeeze()\n        else:\n            predicted_class = torch.LongTensor([class_idx])\n            score = logit[:, class_idx].squeeze()\n\n        logit = f.softmax(logit)\n\n        if torch.cuda.is_available():\n          predicted_class= predicted_class.cuda()\n          score = score.cuda()\n          logit = logit.cuda()\n\n        self.model_arch.zero_grad()\n        score.backward(retain_graph=retain_graph)\n        activations = self.activations['value']\n        b, k, u, v = activations.size()\n        \n        score_saliency_map = torch.zeros((1, 1, h, w))\n\n        if torch.cuda.is_available():\n          activations = activations.cuda()\n          score_saliency_map = score_saliency_map.cuda()\n\n        with torch.no_grad():\n          for i in range(k):\n\n            # upsampling\n            saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n            saliency_map = f.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n\n            if saliency_map.max() == saliency_map.min():\n              continue\n\n            # normalize to 0-1\n            norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n\n            output = self.model_arch(input * norm_saliency_map)\n            output = f.softmax(output)\n            score = output[0][predicted_class]\n\n            score_saliency_map +=  score * saliency_map\n\n        score_saliency_map = f.relu(score_saliency_map)\n        score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n\n        if score_saliency_map_min == score_saliency_map_max:\n            return None\n\n        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n\n        return score_saliency_map\n\n    def __call__(self, input, class_idx=None, retain_graph=False):\n        return self.forward(input, class_idx, retain_graph)\n\n    def metrics(self,model,input_):\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self.forward(input_)\n        if scorecam_map == None:\n            self.predicted_confidence_cam = 0\n        else:\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n        self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n          self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n          self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n\n#ScoreCAM_d\n# class ScoreCAM_d(BaseCAM):\n\n#     \"\"\"\n#         ScoreCAM, inherit from BaseCAM\n\n#     \"\"\"\n\n#     def __init__(self, model_dict):\n#         super().__init__(model_dict)\n#         self.predicted_confidence_cam = 0\n#         self.predicted_confidence_cam_list = []\n#         self.avg_drop = 0\n#         self.avg_increase = 0\n#     def forward(self, input, class_idx=None, retain_graph=False):\n#         b, c, h, w = input.size()\n\n#         # predication on raw input\n#         logit = self.model_arch(input).cuda()\n\n#         if class_idx is None:\n#             predicted_class = logit.max(1)[-1]\n#             score = logit[:, logit.max(1)[-1]].squeeze()\n#         else:\n#             predicted_class = torch.LongTensor([class_idx])\n#             score = logit[:, class_idx].squeeze()\n\n#         logit = f.softmax(logit)\n\n#         if torch.cuda.is_available():\n#           predicted_class= predicted_class.cuda()\n#           score = score.cuda()\n#           logit = logit.cuda()\n\n#         self.model_arch.zero_grad()\n#         score.backward(retain_graph=retain_graph)\n#         activations = self.activations['value']\n#         gradients = self.gradients['value']\n#         b, k, u, v = activations.size()\n        \n#         score_saliency_map = torch.zeros((1, 1, h, w))\n\n#         if torch.cuda.is_available():\n#           activations = activations.cuda()\n#           score_saliency_map = score_saliency_map.cuda()\n\n#         with torch.no_grad():\n#           for i in range(k):\n\n#             # upsampling\n#             saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n#             if (activations[:, i, :, :]*gradients[:, i, :, :]).max() < 0:\n#                 continue\n                \n#             saliency_map = f.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n            \n#             if saliency_map.max() == saliency_map.min():\n#               continue\n\n#             # normalize to 0-1\n#             norm_saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n\n#             output = self.model_arch(input * norm_saliency_map)\n#             output = f.softmax(output)\n#             score = output[0][predicted_class]\n\n#             score_saliency_map +=  score * saliency_map\n\n#         score_saliency_map = f.relu(score_saliency_map)\n#         score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n\n#         if score_saliency_map_min == score_saliency_map_max:\n#             return None\n\n#         score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n\n#         return score_saliency_map\n\n#     def __call__(self, input, class_idx=None, retain_graph=False):\n#         return self.forward(input, class_idx, retain_graph)\n\n#     def metrics(self,model,input_):\n#         predicted_confidence, predicted_class = model(input_).max(1)\n#         predicted_class = predicted_class.item()\n#         predicted_confidence = predicted_confidence.item()\n\n#         scorecam_map = self.forward(input_)\n#         if scorecam_map == None:\n#             self.predicted_confidence_cam = 0\n#         else:\n#             scorecam_map_  = scorecam_map * input_\n#             scorecam_map_output = model(scorecam_map_)\n#             predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n#             self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n#         self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n#         if predicted_confidence > self.predicted_confidence_cam:\n#           self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n#         else:\n#           self.avg_increase = self.avg_increase + 1\n\n#     def percentize(self, count):\n#         self.avg_drop = self.avg_drop*100/count\n#         self.avg_increase = self.avg_increase*100/count\n\n#ScoreCAM_x\nclass ScoreCAM_x(BaseCAM):\n\n    \"\"\"\n    ScoreCAM, inherit from BaseCAM\n\n    \"\"\"\n\n    def __init__(self, model_dict, threshold=0.5, isthreshold=True):\n        super().__init__(model_dict)\n        self.threshold = threshold\n        self.predicted_confidence_cam = 0\n        self.predicted_confidence_cam_list = []\n        self.avg_drop = 0\n        self.avg_increase = 0\n        self.isthreshold = isthreshold\n    def forward(self, input, class_idx=None, retain_graph=False):\n        b, c, h, w = input.size()\n\n        # predication on raw input\n        logit = self.model_arch(input).cuda()\n\n        if class_idx is None:\n            predicted_class = logit.max(1)[-1]\n            score = logit[:, logit.max(1)[-1]].squeeze()\n        else:\n            predicted_class = torch.LongTensor([class_idx])\n            score = logit[:, class_idx].squeeze()\n\n        logit = f.softmax(logit)\n\n        if torch.cuda.is_available():\n            predicted_class= predicted_class.cuda()\n            score = score.cuda()\n            logit = logit.cuda()\n\n        self.model_arch.zero_grad()\n        score.backward(retain_graph=retain_graph)\n        activations = self.activations['value']\n        b, k, u, v = activations.size()\n\n        score_saliency_map = torch.zeros((1, 1, h, w))\n\n        if torch.cuda.is_available():\n            activations = activations.cuda()\n            score_saliency_map = score_saliency_map.cuda()\n\n        with torch.no_grad():\n            for i in range(k):\n\n                # upsampling\n                saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n                saliency_map = f.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n\n                saliency_map = f.tanh(saliency_map)\n                norm_saliency_map = saliency_map.clone()\n\n\n                output = self.model_arch(input * norm_saliency_map)\n                output = f.softmax(output)\n                score = output[0][predicted_class]\n                # print(f'Score is: {score}')   # Score is: tensor([0.0057], device='cuda:0')\n                score_saliency_map +=  score * saliency_map\n                # print(f'Score_saliency map: {score_saliency_map}')\n                score_saliency_map = f.relu(score_saliency_map)\n            score_saliency_map_min, score_saliency_map_max = score_saliency_map.min(), score_saliency_map.max()\n\n        if score_saliency_map_min == score_saliency_map_max:\n            return None\n\n        score_saliency_map = (score_saliency_map - score_saliency_map_min).div(score_saliency_map_max - score_saliency_map_min).data\n\n        return score_saliency_map\n\n    def __call__(self, input, class_idx=None, retain_graph=False):\n        return self.forward(input, class_idx, retain_graph)\n\n    def metrics(self,model,input_):\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self.forward(input_)\n        if scorecam_map == None:\n            self.predicted_confidence_cam = 0\n        else:\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n            self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n            self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n            self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n\n#xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nclass LayerCAM(BaseCAM):\n\n    def __init__(self, model_dict):\n        super().__init__(model_dict)\n        self.predicted_confidence_cam_list = []\n        self.avg_drop = 0\n        self.avg_increase = 0\n    def forward(self, input, class_idx=None, retain_graph=False):\n        b, c, h, w = input.size()\n\n        # predication on raw input\n        logit = self.model_arch(input).cuda()\n\n        if class_idx is None:\n            predicted_class = logit.max(1)[-1]\n            score = logit[:, logit.max(1)[-1]].squeeze()\n        else:\n            predicted_class = torch.LongTensor([class_idx])\n            score = logit[:, class_idx].squeeze()\n\n        #logit = F.softmax(logit)\n\n        if torch.cuda.is_available():\n          predicted_class = predicted_class.cuda()\n          score = score.cuda()\n          logit = logit.cuda()\n\n        one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_()\n        one_hot_output[0][predicted_class] = 1\n        one_hot_output = one_hot_output.cuda(non_blocking=True)\n        # Zero grads\n        self.model_arch.zero_grad()\n        # Backward pass with specified target\n        logit.backward(gradient=one_hot_output, retain_graph=True)\n        activations = self.activations['value'].clone().detach()\n        gradients = self.gradients['value'].clone().detach()\n        b, k, u, v = activations.size()\n\n        with torch.no_grad():\n            activation_maps = activations * f.relu(gradients)\n            cam = torch.sum(activation_maps, dim=1).unsqueeze(0)\n            cam = f.interpolate(cam, size=(h, w), mode='bilinear', align_corners=False)\n            cam_min, cam_max = cam.min(), cam.max()\n            if cam_min == cam_max:\n              return None\n            norm_cam = (cam - cam_min).div(cam_max - cam_min + 1e-8).data\n#\n        return norm_cam\n\n    def __call__(self, input, class_idx=None, retain_graph=False):\n        return self.forward(input, class_idx, retain_graph)\n\n    def metrics(self,model,input_):\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self.forward(input_)\n        if scorecam_map == None:\n            self.predicted_confidence_cam = 0\n        else:\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n        self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n          self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n          self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n","metadata":{"id":"GJOjNBX3iozm","execution":{"iopub.status.busy":"2024-01-07T19:02:10.788533Z","iopub.execute_input":"2024-01-07T19:02:10.789122Z","iopub.status.idle":"2024-01-07T19:02:10.848700Z","shell.execute_reply.started":"2024-01-07T19:02:10.789086Z","shell.execute_reply":"2024-01-07T19:02:10.847616Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as f\n\nfrom statistics import mode, mean\n\n\nclass SaveValues():\n    def __init__(self, m):\n        # register a hook to save values of activations and gradients\n        self.activations = None\n        self.gradients = None\n        self.forward_hook = m.register_forward_hook(self.hook_fn_act)\n        self.backward_hook = m.register_backward_hook(self.hook_fn_grad)\n\n    def hook_fn_act(self, module, input, output):\n        self.activations = output\n\n    def hook_fn_grad(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0]\n\n    def remove(self):\n        self.forward_hook.remove()\n        self.backward_hook.remove()\n\n\nclass CAM(object):\n    \"\"\" Class Activation Mapping \"\"\"\n\n    def __init__(self, model, target_layer):\n        \"\"\"\n        Args:\n            model: a base model to get CAM which have global pooling and fully connected layer.\n            target_layer: conv_layer before Global Average Pooling\n        \"\"\"\n\n        self.model = model\n        self.target_layer = target_layer\n\n        # save values of activations and gradients in target_layer\n        self.values = SaveValues(self.target_layer)\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n        Return:\n            heatmap: class activation mappings of the predicted class\n        \"\"\"\n\n        # object classification\n        score = self.model(x)\n\n        prob = f.softmax(score, dim=1)\n\n        if idx is None:\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            prob = prob.item()\n            # print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n\n        # cam can be calculated from the weights of linear layer and activations\n        weight_fc = list(\n            self.model._modules.get('fc').parameters())[0].to('cpu').data\n\n        cam = self.getCAM(self.values, weight_fc, idx)\n\n        return cam, idx\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def getCAM(self, values, weight_fc, idx):\n        '''\n        values: the activations and gradients of target_layer\n            activations: feature map before GAP.  shape => (1, C, H, W)\n        weight_fc: the weight of fully connected layer.  shape => (num_classes, C)\n        idx: predicted class id\n        cam: class activation map.  shape => (1, num_classes, H, W)\n        '''\n\n        cam = f.conv2d(values.activations, weight=weight_fc[:, :, None, None])\n        _, _, h, w = cam.shape\n\n        # class activation mapping only for the predicted class\n        # cam is normalized with min-max.\n        cam = cam[:, idx, :, :]\n        cam -= torch.min(cam)\n        cam /= torch.max(cam)\n        cam = cam.view(1, 1, h, w)\n\n        return cam.data\n\n\n\nclass GradCAM(CAM):\n    \"\"\" Grad CAM \"\"\"\n\n    def __init__(self, model_dict):\n        model = model_dict['arch'].cuda()\n        model_type = model_dict['type']\n        self.model_arch = model_dict['arch']\n        self.predicted_confidence_cam = 0\n        self.predicted_confidence_cam_list = []\n        self.avg_drop = 0\n        self.avg_increase = 0\n\n        layer_name = model_dict['layer_name']\n        if 'vgg' in model_type.lower():\n            target_layer = find_vgg_layer(self.model_arch, layer_name)\n        elif 'resnet' in model_type.lower():\n            target_layer = find_resnet_layer(self.model_arch, layer_name)\n        elif 'densenet' in model_type.lower():\n            target_layer = find_densenet_layer(self.model_arch, layer_name)\n        elif 'alexnet' in model_type.lower():\n            target_layer = find_alexnet_layer(self.model_arch, layer_name)\n        elif 'squeezenet' in model_type.lower():\n            target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n        elif 'googlenet' in model_type.lower():\n            target_layer = find_googlenet_layer(self.model_arch, layer_name)\n        elif 'shufflenet' in model_type.lower():\n            target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n        elif 'mobilenet' in model_type.lower():\n            target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n        else:\n            self.target_layer = find_layer(self.model_arch, layer_name)\n        super().__init__(model, target_layer)\n\n        \"\"\"\n        Args:\n            model: a base model to get CAM, which need not have global pooling and fully connected layer.\n            target_layer: conv_layer you want to visualize\n        \"\"\"\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n            idx: ground truth index => (1, C)\n        Return:\n            heatmap: class activation mappings of the predicted class\n        \"\"\"\n\n        # anomaly detection\n        _,_,h,w = x.size()\n\n        score = self.model(x)\n#         print(f'Input Size: {x.size()}')\n        prob = f.softmax(score, dim=1)\n\n        if idx is None:\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            prob = prob.item()\n#             print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n\n        # calculate cam of the predicted class\n        cam = self.getGradCAM(self.values, score, idx) # Size of CAM: torch.Size([1, 1, 12, 16])\n        \"\"\"\n        saliency_map = torch.unsqueeze(activations[:, i, :, :], 1)\n        saliency_map = f.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n        \"\"\"\n        if cam == None:\n          return None\n        saliency_map = f.interpolate(cam,size=(h,w), mode='bilinear', align_corners=False)\n        return saliency_map\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def getGradCAM(self, values, score, idx):\n        '''\n        values: the activations and gradients of target_layer\n            activations: feature map before GAP.  shape => (1, C, H, W)\n        score: the output of the model before softmax\n        idx: predicted class id\n        cam: class activation map.  shape=> (1, 1, H, W)\n        '''\n\n        self.model.zero_grad()\n\n        # score[0, idx].backward(retain_graph=True)\n        score[0, idx].backward(retain_graph=False)\n\n        activations = values.activations\n        gradients = values.gradients\n        n, c, _, _ = gradients.shape\n        alpha = gradients.view(n, c, -1).mean(2)\n        alpha = alpha.view(n, c, 1, 1)\n\n        # shape => (1, 1, H', W')\n        cam = (alpha * activations).sum(dim=1, keepdim=True)\n        cam = f.relu(cam)\n        if cam.any() == None:\n          return None\n\n        if torch.min(cam) == torch.max(cam):\n          return None\n        cam -= torch.min(cam)\n        cam /= torch.max(cam)\n        return cam.data\n\n    def metrics(self,model,input_):\n\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self.forward(input_)\n        if scorecam_map == None:\n            self.predicted_confidence_cam = 0\n        else:\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n        self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n          self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n          self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n\nclass GradCAMpp(CAM):\n    \"\"\" Grad CAM plus plus \"\"\"\n\n    def __init__(self, model_dict):\n        model = model_dict['arch'].cuda()\n        model_type = model_dict['type']\n        self.model_arch = model_dict['arch']\n        self.predicted_confidence_cam_list = []\n        self.predicted_confidence_cam = 0\n        self.avg_drop = 0\n        self.avg_increase = 0\n        layer_name = model_dict['layer_name']\n        if 'vgg' in model_type.lower():\n            target_layer = find_vgg_layer(self.model_arch, layer_name)\n        elif 'resnet' in model_type.lower():\n            target_layer = find_resnet_layer(self.model_arch, layer_name)\n        elif 'densenet' in model_type.lower():\n            target_layer = find_densenet_layer(self.model_arch, layer_name)\n        elif 'alexnet' in model_type.lower():\n            target_layer = find_alexnet_layer(self.model_arch, layer_name)\n        elif 'squeezenet' in model_type.lower():\n            target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n        elif 'googlenet' in model_type.lower():\n            target_layer = find_googlenet_layer(self.model_arch, layer_name)\n        elif 'shufflenet' in model_type.lower():\n            target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n        elif 'mobilenet' in model_type.lower():\n            target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n        else:\n            self.target_layer = find_layer(self.model_arch, layer_name)\n        super().__init__(model, target_layer)\n        \"\"\"\n        Args:\n            model: a base model\n            target_layer: conv_layer you want to visualize\n        \"\"\"\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n        Return:\n            heatmap: class activation mappings of predicted classes\n        \"\"\"\n\n        _, _, h, w = x.size()\n\n        # object classification\n        score = self.model(x)\n\n        prob = f.softmax(score, dim=1)\n\n        if idx is None:\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            prob = prob.item()\n#             print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n\n        # caluculate cam of the predicted class\n        cam = self.getGradCAMpp(self.values, score, idx)\n        if cam == None:\n          return None\n        saliency_map = f.interpolate(cam,size=(h,w), mode='bilinear', align_corners=False)\n        return saliency_map\n#         return cam, idx\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def getGradCAMpp(self, values, score, idx):\n        '''\n        values: the activations and gradients of target_layer\n            activations: feature map before GAP.  shape => (1, C, H, W)\n        score: the output of the model before softmax. shape => (1, n_classes)\n        idx: predicted class id\n        cam: class activation map.  shape=> (1, 1, H, W)\n        '''\n        # print((\"Printing from GradCAMpp->getGradCAMpp function:\").upper())\n        self.model.zero_grad()\n\n        # score[0, idx].backward(retain_graph=True)\n        score[0, idx].backward(retain_graph=False)\n\n        activations = values.activations\n        gradients = values.gradients\n        n, c, _, _ = gradients.shape\n\n        # calculate alpha\n        numerator = gradients.pow(2)\n        denominator = 2 * gradients.pow(2)\n        ag = activations * gradients.pow(3)\n        denominator += ag.view(n, c, -1).sum(-1, keepdim=True).view(n, c, 1, 1)\n        denominator = torch.where(\n            denominator != 0.0, denominator, torch.ones_like(denominator))\n        alpha = numerator / (denominator + 1e-7)\n\n        relu_grad = f.relu(score[0, idx].exp() * gradients)\n        # print(f'Alpha is: {alpha}')\n        # print(f'Gradients are: {gradients}')\n        # print(f'Relu_grad are: {relu_grad}')\n        # print(f'Score exp is: {score[0,idx].exp()}')\n        weights = (alpha * relu_grad).view(n, c, -1).sum(-1).view(n, c, 1, 1)\n        # print(f\"Weights are: {weights}\")\n        # shape => (1, 1, H', W')\n        cam = (weights * activations).sum(1, keepdim=True)\n        cam = f.relu(cam)\n        # print(cam)\n        # print(f'Max and min of cam from GradCAMpp are: {torch.max(cam)} and {torch.min(cam)}')\n        if cam.any() == None:\n          return None\n        if torch.min(cam) == torch.max(cam):\n          return None\n        cam -= torch.min(cam)\n        cam /= torch.max(cam)\n\n        return cam.data\n\n    def metrics(self,model,input_):\n\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self.forward(input_)\n        if scorecam_map == None:\n            self.predicted_confidence_cam = 0\n        else:\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n        self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n          self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n          self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n\n        \nclass aug_GradCAMpp(CAM):\n    \"\"\" Grad CAM plus plus \"\"\"\n\n    def __init__(self, model_dict):\n        model = model_dict['arch'].cuda()\n        model_type = model_dict['type']\n        self.model_arch = model_dict['arch']\n        self.predicted_confidence_cam_list = []\n        self.predicted_confidence_cam = 0\n        self.avg_drop = 0\n        self.avg_increase = 0\n        layer_name = model_dict['layer_name']\n        if 'vgg' in model_type.lower():\n            target_layer = find_vgg_layer(self.model_arch, layer_name)\n        elif 'resnet' in model_type.lower():\n            target_layer = find_resnet_layer(self.model_arch, layer_name)\n        elif 'densenet' in model_type.lower():\n            target_layer = find_densenet_layer(self.model_arch, layer_name)\n        elif 'alexnet' in model_type.lower():\n            target_layer = find_alexnet_layer(self.model_arch, layer_name)\n        elif 'squeezenet' in model_type.lower():\n            target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n        elif 'googlenet' in model_type.lower():\n            target_layer = find_googlenet_layer(self.model_arch, layer_name)\n        elif 'shufflenet' in model_type.lower():\n            target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n        elif 'mobilenet' in model_type.lower():\n            target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n        else:\n            self.target_layer = find_layer(self.model_arch, layer_name)\n        super().__init__(model, target_layer)\n        \"\"\"\n        Args:\n            model: a base model\n            target_layer: conv_layer you want to visualize\n        \"\"\"\n\n    def forward(self, x, r=100,idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n        Return:\n            heatmap: class activation mappings of predicted classes\n        \"\"\"\n\n        _, _, h, w = x.size()\n        score = self.model(x)\n\n        prob = f.softmax(score, dim=1)\n\n        if idx is None:\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            prob = prob.item()\n        # caluculate cam of the predicted class\n        transform = transforms.Compose([\n            transforms.RandomAffine(degrees=45, translate=(0.2, 0.2)),\n        ])\n        \n        cam = self.getGradCAMpp(self.values, score, idx)\n        # object classification\n        for idx in range(r):\n            x_ = transform(x)\n            score = self.model(x_)\n            prob = f.softmax(score, dim=1)\n            if idx is None:\n                prob, idx = torch.max(prob, dim=1)\n                idx = idx.item()\n                prob = prob.item()\n            # caluculate cam of the predicted class\n            cam_ = self.getGradCAMpp(self.values, score, idx)\n            if cam_ is None:\n                continue\n            else:\n                cam = cam + cam_\n                \n        if cam == None:\n          return None\n    \n        saliency_map = f.interpolate(cam,size=(h,w), mode='bilinear', align_corners=False)\n        saliency_map = (saliency_map - saliency_map.min())/(saliency_map.max()-saliency_map.min())\n        return saliency_map\n\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def getGradCAMpp(self, values, score, idx):\n        '''\n        values: the activations and gradients of target_layer\n            activations: feature map before GAP.  shape => (1, C, H, W)\n        score: the output of the model before softmax. shape => (1, n_classes)\n        idx: predicted class id\n        cam: class activation map.  shape=> (1, 1, H, W)\n        '''\n        # print((\"Printing from GradCAMpp->getGradCAMpp function:\").upper())\n        self.model.zero_grad()\n\n        # score[0, idx].backward(retain_graph=True)\n        score[0, idx].backward(retain_graph=False)\n\n        activations = values.activations\n        gradients = values.gradients\n        n, c, _, _ = gradients.shape\n\n        # calculate alpha\n        numerator = gradients.pow(2)\n        denominator = 2 * gradients.pow(2)\n        ag = activations * gradients.pow(3)\n        denominator += ag.view(n, c, -1).sum(-1, keepdim=True).view(n, c, 1, 1)\n        denominator = torch.where(\n            denominator != 0.0, denominator, torch.ones_like(denominator))\n        alpha = numerator / (denominator + 1e-7)\n        relu_grad = f.relu(score[0, idx].exp() * gradients)  \n        weights = (alpha * relu_grad).view(n, c, -1).sum(-1).view(n, c, 1, 1)\n        cam = (weights * activations).sum(1, keepdim=True)\n        cam = f.relu(cam)\n        if cam.any() == None:\n          return None\n        if torch.min(cam) == torch.max(cam):\n          return None\n        cam -= torch.min(cam)\n        cam /= torch.max(cam)\n\n        return cam.data\n\n    def metrics(self,model,input_):\n\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self.forward(input_)\n        if scorecam_map == None:\n            self.predicted_confidence_cam = 0\n        else:\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n        self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n          self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n          self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n        \n        \nclass SmoothGradCAMpp(CAM):\n    \"\"\" Smooth Grad CAM plus plus \"\"\"\n\n    def __init__(self, model_dict, n_samples=5, stdev_spread=0.15):\n        model = model_dict['arch'].cuda()\n        model_type = model_dict['type']\n        self.model_arch = model_dict['arch']\n        self.predicted_confidence_cam_list = []\n        self.predicted_confidence_cam = 0\n        self.avg_drop = 0\n        self.avg_increase = 0\n\n        layer_name = model_dict['layer_name']\n        if 'vgg' in model_type.lower():\n            target_layer = find_vgg_layer(self.model_arch, layer_name)\n        elif 'resnet' in model_type.lower():\n            target_layer = find_resnet_layer(self.model_arch, layer_name)\n        elif 'densenet' in model_type.lower():\n            target_layer = find_densenet_layer(self.model_arch, layer_name)\n        elif 'alexnet' in model_type.lower():\n            target_layer = find_alexnet_layer(self.model_arch, layer_name)\n        elif 'squeezenet' in model_type.lower():\n            target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n        elif 'googlenet' in model_type.lower():\n            target_layer = find_googlenet_layer(self.model_arch, layer_name)\n        elif 'shufflenet' in model_type.lower():\n            target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n        elif 'mobilenet' in model_type.lower():\n            target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n        else:\n            self.target_layer = find_layer(self.model_arch, layer_name)\n        super().__init__(model, target_layer)\n        \"\"\"\n        Args:\n            model: a base model\n            target_layer: conv_layer you want to visualize\n            n_sample: the number of samples\n            stdev_spread: standard deviation\n        \"\"\"\n\n        self.n_samples = n_samples\n        self.stdev_spread = stdev_spread\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n        Return:\n            heatmap: class activation mappings of predicted classes\n        \"\"\"\n\n        _, _, h, w = x.size()\n        stdev = self.stdev_spread / (x.max() - x.min())\n        std_tensor = torch.ones_like(x) * stdev\n\n        indices = []\n        probs = []\n\n        for i in range(self.n_samples):\n            self.model.zero_grad()\n\n            x_with_noise = torch.normal(mean=x, std=std_tensor)\n            x_with_noise.requires_grad_()\n\n            score = self.model(x_with_noise)\n\n            prob = f.softmax(score, dim=1)\n\n            if idx is None:\n                prob, idx = torch.max(prob, dim=1)\n                idx = idx.item()\n                probs.append(prob.item())\n\n            indices.append(idx)\n\n            score[0, idx].backward(retain_graph=True)\n            if i == self.n_samples-1:\n#                 print('Limit Accessed')\n                score[0, idx].backward(retain_graph=False)\n\n            activations = self.values.activations\n            gradients = self.values.gradients\n            n, c, _, _ = gradients.shape\n\n            # calculate alpha\n            numerator = gradients.pow(2)\n            denominator = 2 * gradients.pow(2)\n            ag = activations * gradients.pow(3)\n            denominator += \\\n                ag.view(n, c, -1).sum(-1, keepdim=True).view(n, c, 1, 1)\n            denominator = torch.where(\n                denominator != 0.0, denominator, torch.ones_like(denominator))\n            alpha = numerator / (denominator + 1e-7)\n\n            relu_grad = f.relu(score[0, idx].exp() * gradients)\n            weights = \\\n                (alpha * relu_grad).view(n, c, -1).sum(-1).view(n, c, 1, 1)\n\n            # shape => (1, 1, H', W')\n            cam = (weights * activations).sum(1, keepdim=True)\n            cam = f.relu(cam)\n            cam -= torch.min(cam)\n            cam /= torch.max(cam)\n\n            if i == 0:\n                total_cams = cam.clone()\n            else:\n                total_cams += cam\n\n        total_cams /= self.n_samples\n        idx = mode(indices)\n        prob = mean(probs)\n        if total_cams.any() == None:\n          return None\n        if total_cams.max() == total_cams.min():\n          return None\n#         print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n        saliency_map = f.interpolate(total_cams,size=(h,w), mode='bilinear', align_corners=False)\n        return saliency_map\n#         return total_cams.data, idx\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def metrics(self,model,input_):\n\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self.forward(input_)\n        if scorecam_map == None:\n            self.predicted_confidence_cam = 0\n        else:\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n        self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n          self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n          self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n","metadata":{"id":"KoPjzCuMiozm","execution":{"iopub.status.busy":"2024-01-07T19:02:10.855403Z","iopub.execute_input":"2024-01-07T19:02:10.855775Z","iopub.status.idle":"2024-01-07T19:02:10.951858Z","shell.execute_reply.started":"2024-01-07T19:02:10.855744Z","shell.execute_reply":"2024-01-07T19:02:10.950938Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# X GradCAM Implementation:","metadata":{"id":"7fPy9woXiozn"}},{"cell_type":"code","source":"import argparse\nimport cv2\nimport numpy as np\nimport torch\nfrom torch.autograd import Function\nfrom torchvision import models\n\nclass _BaseCAM(object):\n    \"\"\" Base class for Class activation mapping.\n\n        : Args\n            - **model_dict -** : Dict. Has format as dict(type='vgg', arch=torchvision.models.vgg16(pretrained=True),\n            layer_name='features',input_size=(224, 224)).\n\n    \"\"\"\n\n    def __init__(self, model_dict):\n        model_type = model_dict['type']\n        layer_name = model_dict['layer_name']\n\n        self.model_arch = model_dict['arch']\n        self.model_arch.eval()\n        if torch.cuda.is_available():\n          self.model_arch.cuda()\n        self.gradients = dict()\n        self.activations = dict()\n\n        def backward_hook(module, grad_input, grad_output):\n            if torch.cuda.is_available():\n              self.gradients['value'] = grad_output[0].cuda()\n            else:\n              self.gradients['value'] = grad_output[0]\n            return None\n\n        def forward_hook(module, input, output):\n            if torch.cuda.is_available():\n              self.activations['value'] = output.cuda()\n            else:\n              self.activations['value'] = output\n            return None\n\n        if 'vgg' in model_type.lower():\n            self.target_layer = find_vgg_layer(self.model_arch, layer_name)\n        elif 'resnet' in model_type.lower():\n            self.target_layer = find_resnet_layer(self.model_arch, layer_name)\n        elif 'densenet' in model_type.lower():\n            self.target_layer = find_densenet_layer(self.model_arch, layer_name)\n        elif 'alexnet' in model_type.lower():\n            self.target_layer = find_alexnet_layer(self.model_arch, layer_name)\n        elif 'squeezenet' in model_type.lower():\n            self.target_layer = find_squeezenet_layer(self.model_arch, layer_name)\n        elif 'googlenet' in model_type.lower():\n            self.target_layer = find_googlenet_layer(self.model_arch, layer_name)\n        elif 'shufflenet' in model_type.lower():\n            self.target_layer = find_shufflenet_layer(self.model_arch, layer_name)\n        elif 'mobilenet' in model_type.lower():\n            self.target_layer = find_mobilenet_layer(self.model_arch, layer_name)\n        else:\n            self.target_layer = find_layer(self.model_arch, layer_name)\n\n        self.target_layer.register_forward_hook(forward_hook)\n        self.target_layer.register_backward_hook(backward_hook)\n\n    def forward(self, input, class_idx=None, retain_graph=True):\n        return None\n\n    def __call__(self, input, class_idx=None, retain_graph=True):\n        return self.forward(input, class_idx, retain_graph)\n\n    def get_output(self, x):\n      for name, module in self.model_arch._modules.items():\n            x = module(x)\n      return x\n\n# class ModelOutputs(_BaseCAM):\n#     \"\"\" Class for making a forward pass, and getting:\n#     1. The network output.\n#     2. Activations from intermeddiate targetted layers.\n#     3. Gradients from intermeddiate targetted layers. \"\"\"\n\n#     def __init__(self, model_dict):\n#         super().__init__(model_dict)\n#         self.model = model_dict['arch']\n\n\n#         # self.feature_extractor = _BaseCAM(model_dict)\n\n#     def get_gradients(self):\n#         return self.gradients['value']\n\n#     def __call__(self, x):\n#         print(f'Self activations: {self.activations}')\n\n#         target_activations = self.activations['value']\n#         output = self.get_output(x)\n#         output = output.view(output.size(0), -1)\n#         output = self.model.classifier(output)\n#         return target_activations, output\n\n\n\nclass XGradCam(_BaseCAM):\n    def __init__(self, model_dict,use_cuda=True):\n        super().__init__(model_dict)\n        self.model = model_dict['arch']\n        self.model.eval()\n        self.cuda = use_cuda\n        self.predicted_confidence_cam_list = []\n        self.predicted_confidence_cam = 0\n        self.avg_drop = 0\n        self.avg_increase = 0\n        if self.cuda:\n            self.model = self.model.cuda()\n\n        # self.extractor = ModelOutputs(model_dict)\n\n    def forward(self, input):\n        return self.model(input)\n\n    def __call__(self, input, index=-1):\n        output = self.model(input)\n        # for name, module in self.model._modules.items():\n        #     output = module(output)\n\n        # output = output.view(output.size(0), -1)\n        # output = self.model.classifier(output)\n        # output.backward()\n        features = self.activations['value']\n        if self.cuda:\n            features = features.cuda()\n            output = output.cuda()\n\n\n        if index == -1:\n            index = np.argmax(output.cpu().data.numpy())\n\n        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n        one_hot[0][index] = 1\n        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n        if self.cuda:\n            one_hot = torch.sum(one_hot.cuda() * output)\n        else:\n            one_hot = torch.sum(one_hot * output)\n\n        # self.model.features.zero_grad()\n        # self.model.classifier.zero_grad()\n        self.model.zero_grad()\n        one_hot.backward(retain_graph=True)\n\n        grads_val = self.gradients['value'].cpu().data.numpy()\n\n        target = features\n        target = target.cpu().data.numpy()[0, :]\n\n        # XGrad_CAM\n        X_weights = np.sum(grads_val[0, :] * target, axis=(1, 2))\n        X_weights = X_weights / (np.sum(target, axis=(1, 2)) + 1e-6)\n        # Grad_CAM\n        weights = np.mean(grads_val, axis=(2, 3))[0, :]\n\n        X_cam = np.zeros(target.shape[1:], dtype=np.float32)\n        cam = np.zeros(target.shape[1:], dtype=np.float32)\n\n        for i, w in enumerate(weights):\n            cam += w * target[i, :, :]\n            X_cam += X_weights[i] * target[i, :, :]\n\n        cam = np.maximum(cam, 0)\n        cam = cv2.resize(cam, (224, 224))\n        cam = cam - np.min(cam)\n        cam = cam / np.max(cam)\n        X_cam = np.maximum(X_cam, 0)\n        X_cam = cv2.resize(X_cam, (224, 224))\n        if X_cam.any() == None:\n          return None\n        if np.min(X_cam) == np.max(X_cam):\n          return None\n        X_cam = X_cam - np.min(X_cam)\n        X_cam = X_cam / np.max(X_cam)\n        return  X_cam\n\n    def metrics(self,model,input_):\n\n        predicted_confidence, predicted_class = model(input_).max(1)\n        predicted_class = predicted_class.item()\n        predicted_confidence = predicted_confidence.item()\n\n        scorecam_map = self(input_)\n        # print(f'Saliency map size: {scorecam_map.size()}')\n        if scorecam_map.any() == None:\n            self.predicted_confidence_cam = 0\n        else:\n            # print(f'Input size: {input_.size()}')\n            scorecam_map =  torch.from_numpy(scorecam_map).cuda()\n            scorecam_map_  = scorecam_map * input_\n            scorecam_map_output = model(scorecam_map_)\n            predicted_confidence_cam = scorecam_map_output[0][predicted_class]\n            self.predicted_confidence_cam = predicted_confidence_cam.item()\n\n        self.predicted_confidence_cam_list.append(self.predicted_confidence_cam)\n        if predicted_confidence > self.predicted_confidence_cam:\n          self.avg_drop = self.avg_drop + (predicted_confidence - self.predicted_confidence_cam)/predicted_confidence\n        else:\n          self.avg_increase = self.avg_increase + 1\n\n    def percentize(self, count):\n        self.avg_drop = self.avg_drop*100/count\n        self.avg_increase = self.avg_increase*100/count\n","metadata":{"id":"ZXjzCRQdiozo","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-01-07T19:02:10.953861Z","iopub.execute_input":"2024-01-07T19:02:10.954170Z","iopub.status.idle":"2024-01-07T19:02:10.986562Z","shell.execute_reply.started":"2024-01-07T19:02:10.954138Z","shell.execute_reply":"2024-01-07T19:02:10.985645Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print('ll')\n\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as transforms\n\n\nclass Data(Dataset):\n    #image_dir will contain the path to imagenet-mini->val directory->The directory with folders for different classes and images in them\n    def __init__(self, image_dir = '/kaggle/input/imagenetmini-1000/imagenet-mini/val'):\n        self.image_dir = image_dir\n        self.images = []\n        for item in os.listdir(image_dir):\n            path = os.path.join(image_dir+'/'+item)\n            self.images_ = os.listdir(path)\n            for images in self.images_:\n                path_ = os.path.join(path+'/'+images)\n                self.images.append(path_)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.image_dir, self.images[index])\n        image = load_image(img_path)\n        # print(f'From Data: {image.shape}')\n        image = apply_transforms(image)\n        return image","metadata":{"id":"_D_6kWpfiozp","execution":{"iopub.status.busy":"2024-01-07T19:02:10.987733Z","iopub.execute_input":"2024-01-07T19:02:10.988070Z","iopub.status.idle":"2024-01-07T19:02:10.999855Z","shell.execute_reply.started":"2024-01-07T19:02:10.988037Z","shell.execute_reply":"2024-01-07T19:02:10.998989Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"ll\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Testing the CAMs**","metadata":{"id":"IkyW1X6fiozp"}},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n# import torchvision.models as models\n# from tqdm import tqdm\n# import pandas as pd\n# import warnings\n\n# warnings.filterwarnings('ignore')\n\n# #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# #VGG\n# vgg = models.vgg19(pretrained=True).eval()\n# vgg_model_dict = dict(type='vgg16', arch=vgg, layer_name='features_29',input_size=(224, 224))\n# vgg_xgradcam = XGradCam(vgg_model_dict)\n# #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# def metrics(dataloader):\n\n#   loop = tqdm(dataloader)\n# #   loop = dataloader   #for when I don't need tqdm\n#   count = 0\n\n#   for idx, data in enumerate(loop):\n#     input_ = data\n#     input_ = input_.squeeze()\n#     input_ = input_.unsqueeze(0)\n\n#     if torch.cuda.is_available():\n#       input_ = input_.cuda()\n#     count += 1\n#     print(f'Input Size:{input_.size()}')\n#     saliency_map = vgg_xgradcam(input_)\n#     saliency_map =  torch.from_numpy(saliency_map)\n#     print(f'Saliency Map: {saliency_map.shape}')\n#     e_map = saliency_map * input_.cpu()\n#     print(f'E-Map Size: {e_map.size()}')\n#     if count == 1:\n#         break\n\n\n\n\n# val_ds = Data()\n# val_loader = DataLoader(\n#       val_ds,\n#       batch_size=1,\n#       shuffle=True,\n#   )\n\n# metrics(val_loader)","metadata":{"id":"USj-yX_Yiozp","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-01-07T19:02:11.001853Z","iopub.execute_input":"2024-01-07T19:02:11.002139Z","iopub.status.idle":"2024-01-07T19:02:11.012086Z","shell.execute_reply.started":"2024-01-07T19:02:11.002116Z","shell.execute_reply":"2024-01-07T19:02:11.011227Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n# import torchvision.models as models\n# from tqdm import tqdm\n# import pandas as pd\n# import warnings\n\n# warnings.filterwarnings('ignore')\n\n# #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# #VGG\n# vgg = models.vgg19(pretrained=True).eval()\n# vgg_model_dict = dict(type='vgg16', arch=vgg, layer_name='features_29',input_size=(224, 224))\n\n\n# vgg_scorecam_5 = ScoreCAM_x(vgg_model_dict, threshold=0.5)\n\n# #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# #ResNet\n# resnet = models.resnet18(pretrained=True).eval()\n# resnet_model_dict = dict(type='resnet18', arch=resnet, layer_name='layer4',input_size=(224, 224))\n\n\n# resnet_scorecam_5 = ScoreCAM_x(resnet_model_dict, threshold=0.5)\n\n# #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# vgg_d = {'scorecam_0': 0,'scorecam_5': 0,'scorecam_6': 0,'scorecam_7': 0, 'scorecam_8': 0 }\n# resnet_d = {'scorecam_0':0,'scorecam_5': 0,'scorecam_6': 0,'scorecam_7': 0, 'scorecam_8': 0 }\n# #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# def metrics(dataloader):\n\n#   loop = tqdm(dataloader)\n# #   loop = dataloader   #for when I don't need tqdm\n#   count = 0\n\n#   for idx, data in enumerate(loop):\n#     input_ = data\n#     input_ = input_.squeeze()\n#     input_ = input_.unsqueeze(0)\n\n#     if torch.cuda.is_available():\n#       input_ = input_.cuda()\n#     count += 1\n\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n#     vgg_scorecam_5.metrics(vgg,input_)\n\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#     vgg_d['scorecam_0'] = vgg_scorecam_0.predicted_confidence_cam\n#     vgg_d['scorecam_5'] = vgg_scorecam_5.predicted_confidence_cam\n#     vgg_d['scorecam_6'] = vgg_scorecam_6.predicted_confidence_cam\n#     vgg_d['scorecam_7'] = vgg_scorecam_7.predicted_confidence_cam\n#     vgg_d['scorecam_8'] = vgg_scorecam_8.predicted_confidence_cam\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#     v = list(vgg_d.values())\n#     k = list(vgg_d.keys())\n#     n = k[v.index(max(v))]\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#     if n == 'scorecam_0':\n#        vgg_scorecam_0.win += 1\n#     elif n == 'scorecam_5':\n#        vgg_scorecam_5.win += 1\n#     elif n == 'scorecam_6':\n#        vgg_scorecam_6.win += 1\n#     elif n == 'scorecam_7':\n#        vgg_scorecam_7.win += 1\n#     else:\n#        vgg_scorecam_8.win += 1\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#     resnet_scorecam_0.metrics(resnet, input_)\n#     resnet_scorecam_5.metrics(resnet, input_)\n#     resnet_scorecam_6.metrics(resnet, input_)\n#     resnet_scorecam_7.metrics(resnet, input_)\n#     resnet_scorecam_8.metrics(resnet, input_)\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#     resnet_d['scorecam_0'] = vgg_scorecam_0.predicted_confidence_cam\n# #     resnet_d['scorecam_3'] = vgg_scorecam_3.predicted_confidence_cam\n# #     resnet_d['scorecam_4'] = vgg_scorecam_4.predicted_confidence_cam\n#     resnet_d['scorecam_5'] = vgg_scorecam_5.predicted_confidence_cam\n#     resnet_d['scorecam_6'] = vgg_scorecam_6.predicted_confidence_cam\n#     resnet_d['scorecam_7'] = vgg_scorecam_7.predicted_confidence_cam\n#     resnet_d['scorecam_8'] = vgg_scorecam_8.predicted_confidence_cam\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#     v = list(resnet_d.values())\n#     k = list(resnet_d.keys())\n#     n = k[v.index(max(v))]\n#     #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#     if n == 'scorecam_0':\n#        resnet_scorecam_0.win += 1\n#     elif n == 'scorecam_5':\n#        resnet_scorecam_5.win += 1\n#     elif n == 'scorecam_6':\n#        resnet_scorecam_6.win += 1\n#     elif n == 'scorecam_7':\n#        resnet_scorecam_7.win += 1\n#     else:\n#        resnet_scorecam_8.win += 1\n#     if count == 10:\n#         break\n\n\n\n#   vgg_scorecam_0.percentize(count)\n#   vgg_scorecam_5.percentize(count)\n#   vgg_scorecam_6.percentize(count)\n#   vgg_scorecam_7.percentize(count)\n#   vgg_scorecam_8.percentize(count)\n\n#   resnet_scorecam_0.percentize(count)\n#   resnet_scorecam_5.percentize(count)\n#   resnet_scorecam_6.percentize(count)\n#   resnet_scorecam_7.percentize(count)\n#   resnet_scorecam_8.percentize(count)\n\n# val_ds = Data()\n# val_loader = DataLoader(\n#       val_ds,\n#       batch_size=1,\n#       shuffle=True,\n#   )\n\n# metrics(val_loader)\n\n\n# data = {'Model':['scorecam0','scorecam_5','scorecam_6','scorecam_7','scorecam_8'],\n#       'Average Drop':[vgg_scorecam_0.avg_drop,vgg_scorecam_5.avg_drop,vgg_scorecam_6.avg_drop,\n#                       vgg_scorecam_7.avg_drop,vgg_scorecam_8.avg_drop],\n#       'Average Increase':[vgg_scorecam_0.avg_increase,vgg_scorecam_5.avg_increase,vgg_scorecam_6.avg_increase,\n#                           vgg_scorecam_7.avg_increase,vgg_scorecam_8.avg_increase],\n#         'Win':[vgg_scorecam_0.win,vgg_scorecam_5.win,vgg_scorecam_6.win,vgg_scorecam_7.win,vgg_scorecam_8.win]}\n\n# df = pd.DataFrame(data)\n# print('Vgg details\\n')\n# print(df)\n# # saving the dataframe\n# df.to_csv('vgg_scorecam_ablation_study.csv')\n\n\n# data = {'Model':['scorecam_0','scorecam_5','scorecam_6','scorecam_7','scorecam_8'],\n#       'Average Drop':[resnet_scorecam_0.avg_drop,resnet_scorecam_5.avg_drop,\n#                       resnet_scorecam_6.avg_drop,resnet_scorecam_7.avg_drop,resnet_scorecam_8.avg_drop],\n#       'Average Increase':[resnet_scorecam_0.avg_increase,\n#                           resnet_scorecam_5.avg_increase,resnet_scorecam_6.avg_increase,\n#                           resnet_scorecam_7.avg_increase,resnet_scorecam_8.avg_increase],\n#        'Win':[resnet_scorecam_0.win,resnet_scorecam_5.win,resnet_scorecam_6.win,\n#                resnet_scorecam_7.win,resnet_scorecam_8.win]}\n# df = pd.DataFrame(data)\n# print('\\n\\nResnet Details\\n')\n# print(df)\n# # saving the dataframe\n# df.to_csv('resnet_scorecam_ablation_study.csv')","metadata":{"id":"OfuKgJIDiozp","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-01-07T19:02:11.013148Z","iopub.execute_input":"2024-01-07T19:02:11.013428Z","iopub.status.idle":"2024-01-07T19:02:11.025757Z","shell.execute_reply.started":"2024-01-07T19:02:11.013396Z","shell.execute_reply":"2024-01-07T19:02:11.024855Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print('ll')\n\nfrom torch.utils.data import DataLoader\nimport torchvision.models as models\nfrom tqdm import tqdm\nimport pandas as pd\nimport warnings\n\n\n\nwarnings.filterwarnings('ignore')\nval_ds = Data()\nval_loader = DataLoader(\n      val_ds,\n      batch_size=1,\n      shuffle=True,\n  )\n\ndef metrics(model,cam_model_list=[],dataloader=val_loader):\n\n  loop = tqdm(dataloader)\n  # loop = dataloader   #for when I don't need tqdm\n  count = 0\n\n  for idx, data in enumerate(loop):\n    input_ = data\n    input_ = input_.squeeze()\n    input_ = input_.unsqueeze(0)\n    input = input_.type(torch.float16)\n    if torch.cuda.is_available():\n      input_ = input_.cuda()\n    count += 1\n    #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n    for cam_model in cam_model_list:\n        cam_model.metrics(model, input_)\n    if count == 50:\n        break\n  \n  for cam_model in cam_model_list:\n      cam_model.percentize(count)","metadata":{"id":"W8Q9GwWDBVbF","execution":{"iopub.status.busy":"2024-01-07T19:02:11.026892Z","iopub.execute_input":"2024-01-07T19:02:11.027164Z","iopub.status.idle":"2024-01-07T19:02:11.442766Z","shell.execute_reply.started":"2024-01-07T19:02:11.027134Z","shell.execute_reply":"2024-01-07T19:02:11.441777Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"ll\n","output_type":"stream"}]},{"cell_type":"code","source":"print('ll')\n\n\nvgg_avg_drop = {'gradcam':0,'gradcampp':0,'xgradcam':0,'layercam':0,'scorecam':0,'scorecam_x':0,'augpp':0}\nvgg_avg_increase = {'gradcam':0,'gradcampp':0,'xgradcam':0,'layercam':0,'scorecam':0,'scorecam_x':0,'augpp':0}\nvgg_avg_win = {'gradcam':0,'gradcampp':0,'xgradcam':0,'layercam':0,'scorecam':0,'scorecam_x':0,'augpp':0}\nvgg_predicted_confidence = {'gradcam':[],'gradcampp':[],'xgradcam':[],'layercam':[],\n                            'scorecam':[],'scorecam_x':[],'augpp':[]}\n\n# #xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n#VGG\nvgg = models.vgg19(pretrained=True).eval()\nvgg_model_dict = dict(type='vgg16', arch=vgg, layer_name='features_36',input_size=(224, 224))\n\nvgg_gradcam = GradCAM(vgg_model_dict)\nvgg_gradcampp = GradCAMpp(vgg_model_dict)\nvgg_xgradcam = XGradCam(vgg_model_dict)\nvgg_layercam = LayerCAM(vgg_model_dict)\nvgg_scorecam = ScoreCAM(vgg_model_dict)\nvgg_scorecam_x = ScoreCAM_x(vgg_model_dict, isthreshold=True)\n# vgg_scorecam_d = ScoreCAM_d(vgg_model_dict)\nvgg_augpp = aug_GradCAMpp(vgg_model_dict)\n\nmodel_list = [vgg_gradcam, vgg_gradcampp, vgg_xgradcam, vgg_layercam, vgg_scorecam, vgg_scorecam_x, vgg_augpp]\n# model_list = [vgg_scorecam, vgg_scorecam_x]\n\nmetrics(vgg, model_list)\n\nvgg_predicted_confidence['gradcam'] = vgg_gradcam.predicted_confidence_cam_list[:]\nvgg_avg_drop['gradcam'] = vgg_gradcam.avg_drop\nvgg_avg_increase['gradcam'] = vgg_gradcam.avg_increase\ndel vgg_gradcam\n\nvgg_predicted_confidence['gradcampp'] = vgg_gradcampp.predicted_confidence_cam_list[:]\nvgg_avg_drop['gradcampp'] = vgg_gradcampp.avg_drop\nvgg_avg_increase['gradcampp'] = vgg_gradcampp.avg_increase\ndel vgg_gradcampp\n\nvgg_predicted_confidence['xgradcam'] = vgg_xgradcam.predicted_confidence_cam_list[:]\nvgg_avg_drop['xgradcam'] = vgg_xgradcam.avg_drop\nvgg_avg_increase['xgradcam'] = vgg_xgradcam.avg_increase\ndel vgg_xgradcam\n\nvgg_predicted_confidence['layercam'] = vgg_layercam.predicted_confidence_cam_list[:]\nvgg_avg_drop['layercam'] = vgg_layercam.avg_drop\nvgg_avg_increase['layercam'] = vgg_layercam.avg_increase\ndel vgg_layercam\n\nvgg_predicted_confidence['scorecam'] = vgg_scorecam.predicted_confidence_cam_list[:]\nvgg_avg_drop['scorecam'] = vgg_scorecam.avg_drop\nvgg_avg_increase['scorecam'] = vgg_scorecam.avg_increase\ndel vgg_scorecam\n\nvgg_predicted_confidence['scorecam_x'] = vgg_scorecam_x.predicted_confidence_cam_list[:]\nvgg_avg_drop['scorecam_x'] = vgg_scorecam_x.avg_drop\nvgg_avg_increase['scorecam_x'] = vgg_scorecam_x.avg_increase\ndel vgg_scorecam_x\n\n# vgg_predicted_confidence['scorecam_d'] = vgg_scorecam_d.predicted_confidence_cam_list[:]\n# vgg_avg_drop['scorecam_d'] = vgg_scorecam_d.avg_drop\n# vgg_avg_increase['scorecam_d'] = vgg_scorecam_d.avg_increase\n# del vgg_scorecam_d\n\nvgg_predicted_confidence['augpp'] = vgg_augpp.predicted_confidence_cam_list[:]\nvgg_avg_drop['augpp'] = vgg_augpp.avg_drop\nvgg_avg_increase['augpp'] = vgg_augpp.avg_increase\ndel vgg_augpp\n    \nprint('Completed Successfully')\n\n\n#xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# ResNet\nresnet = models.resnet18(pretrained=True).eval()\nresnet_model_dict = dict(type='resnet18', arch=resnet, layer_name='layer4',input_size=(224, 224))\n\nresnet_avg_drop = {'gradcam':0,'gradcampp':0,'xgradcam':0,'layercam':0,\n                   'scorecam':0,'scorecam_x':0,'augpp':0}\nresnet_avg_increase = {'gradcam':0,'gradcampp':0,'xgradcam':0,'layercam':0,\n                       'scorecam':0,'scorecam_x':0,'augpp':0}\nresnet_avg_win = {'gradcam':0,'gradcampp':0,'xgradcam':0,'layercam':0,\n                  'scorecam':0,'scorecam_x':0,'augpp':0}\nresnet_predicted_confidence = {'gradcam':[],'gradcampp':[],'xgradcam':[],'layercam':[],\n                               'scorecam':[],'scorecam_x':[],'augpp':[]}\n\n\n\nresnet_gradcam = GradCAM(resnet_model_dict)\nresnet_gradcampp = GradCAMpp(resnet_model_dict)\nresnet_xgradcam = XGradCam(resnet_model_dict)\nresnet_layercam = LayerCAM(resnet_model_dict)\nresnet_scorecam = ScoreCAM(resnet_model_dict)\nresnet_scorecam_x = ScoreCAM_x(resnet_model_dict, isthreshold=True)\n# resnet_scorecam_d = ScoreCAM_d(resnet_model_dict)\nresnet_augpp = aug_GradCAMpp(resnet_model_dict)\n\nmodel_list = [resnet_gradcam,resnet_gradcampp,resnet_xgradcam ,resnet_layercam, resnet_scorecam ,resnet_scorecam_x,\n             resnet_augpp]\n# model_list = [resnet_scorecam ,resnet_scorecam_x]\n\nmetrics(resnet,model_list)\n\nresnet_predicted_confidence['gradcam'] = resnet_gradcam.predicted_confidence_cam_list[:]\nresnet_avg_drop['gradcam'] = resnet_gradcam.avg_drop\nresnet_avg_increase['gradcam'] = resnet_gradcam.avg_increase\ndel resnet_gradcam\n\nresnet_predicted_confidence['gradcampp'] = resnet_gradcampp.predicted_confidence_cam_list[:]\nresnet_avg_drop['gradcampp'] = resnet_gradcampp.avg_drop\nresnet_avg_increase['gradcampp'] = resnet_gradcampp.avg_increase\ndel resnet_gradcampp\n\nresnet_predicted_confidence['xgradcam'] = resnet_xgradcam.predicted_confidence_cam_list[:]\nresnet_avg_drop['xgradcam'] = resnet_xgradcam.avg_drop\nresnet_avg_increase['xgradcam'] = resnet_xgradcam.avg_increase\ndel resnet_xgradcam\n\nresnet_predicted_confidence['layercam'] = resnet_layercam.predicted_confidence_cam_list[:]\nresnet_avg_drop['layercam'] = resnet_layercam.avg_drop\nresnet_avg_increase['layercam'] = resnet_layercam.avg_increase\ndel resnet_layercam\n\nresnet_predicted_confidence['scorecam'] = resnet_scorecam.predicted_confidence_cam_list[:]\nresnet_avg_drop['scorecam'] = resnet_scorecam.avg_drop\nresnet_avg_increase['scorecam'] = resnet_scorecam.avg_increase\ndel resnet_scorecam\n\nresnet_predicted_confidence['scorecam_x'] = resnet_scorecam_x.predicted_confidence_cam_list[:]\nresnet_avg_drop['scorecam_x'] = resnet_scorecam_x.avg_drop\nresnet_avg_increase['scorecam_x'] = resnet_scorecam_x.avg_increase\ndel resnet_scorecam_x\n\n# resnet_predicted_confidence['scorecam_d'] = resnet_scorecam_d.predicted_confidence_cam_list[:]\n# resnet_avg_drop['scorecam_d'] = resnet_scorecam_d.avg_drop\n# resnet_avg_increase['scorecam_d'] = resnet_scorecam_d.avg_increase\n# del resnet_scorecam_d\n\nresnet_predicted_confidence['augpp'] = resnet_augpp.predicted_confidence_cam_list[:]\nresnet_avg_drop['augpp'] = resnet_augpp.avg_drop\nresnet_avg_increase['augpp'] = resnet_augpp.avg_increase\ndel resnet_augpp","metadata":{"id":"eQwqrc9SKcm3","outputId":"6d2d3320-4909-4de6-deb7-181d448a41f7","execution":{"iopub.status.busy":"2024-01-07T19:02:11.444098Z","iopub.execute_input":"2024-01-07T19:02:11.444375Z","iopub.status.idle":"2024-01-07T19:11:40.639584Z","shell.execute_reply.started":"2024-01-07T19:02:11.444352Z","shell.execute_reply":"2024-01-07T19:11:40.638562Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"ll\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 49/3923 [06:15<8:14:32,  7.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"Completed Successfully\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 49/3923 [03:11<4:12:47,  3.92s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndef vgg_win_metrics(names_exclude=[],predicted_confidence_dict=vgg_predicted_confidence, filename='vgg_metrics_scorecam.csv'):\n  keys = list(predicted_confidence_dict.keys())\n  vgg_avg_win_keys = list(vgg_avg_win.keys())\n  num_total = len(predicted_confidence_dict[keys[0]])\n#   num_total = 20\n  names = ['gradcam','gradcampp','xgradcam','layercam','scorecam',\n          'scorecam_x', 'augpp']\n#   names = ['scorecam','scorecam_x']\n  max_conf = -1000\n  max_conf_item = 'k'\n    \n  for item in vgg_avg_win:\n    vgg_avg_win[item] = 0\n    \n  for idx in range(num_total):\n    for index, item in enumerate(names):\n       if (item in names_exclude):\n          continue\n#        print(item)\n       if predicted_confidence_dict[item][idx] > max_conf:\n          max_conf = predicted_confidence_dict[item][idx]\n          max_conf_item = item\n\n    vgg_avg_win[max_conf_item] += 1\n    max_conf = -1000\n    max_conf_item = ''\n\n  for item in vgg_avg_win:\n    vgg_avg_win[item] = vgg_avg_win[item]*100/num_total\n    \n  average_drop = []\n  average_increase = []\n  average_win = []\n  names_ = []\n  for item in names:\n    if (item in names_exclude):\n        continue\n    names_.append(item)\n    average_drop.append(vgg_avg_drop[item])\n    average_increase.append(vgg_avg_increase[item])\n    average_win.append(vgg_avg_win[item])\n  \n  data = {'name':names_,'Average_drop':average_drop, 'Average_increase':average_increase,'Average_win':average_win}\n  df = pd.DataFrame(data)\n  df.to_csv(filename)\n  print(filename)\n  print(df)\n\ndef resnet_win_metrics(names_exclude=[],predicted_confidence_dict = resnet_predicted_confidence, filename='resnet_metrics_scorecam.csv'):\n  keys = list(predicted_confidence_dict.keys())\n  num_keys = len(keys)\n  num_total = len(predicted_confidence_dict[keys[0]])\n#   num_total = 20\n  max_conf = -1000\n  max_conf_item  = 'k'\n  names = ['gradcam','gradcampp','xgradcam','layercam','scorecam','scorecam_x','augpp']\n\n#   names = ['scorecam','scorecam_x']\n    \n  for item in resnet_avg_win:\n    resnet_avg_win[item] = 0\n    \n  for idx in range(num_total):\n    for index, item in enumerate(names):\n         if (item in names_exclude):\n            continue\n         if predicted_confidence_dict[item][idx] > max_conf:\n            max_conf = predicted_confidence_dict[item][idx]\n            max_conf_item = item\n\n    resnet_avg_win[max_conf_item] += 1\n    max_conf = -1000\n    max_conf_item = 'k' \n    \n  for item in resnet_avg_win:   \n    resnet_avg_win[item] = resnet_avg_win[item]*100/num_total\n  \n  \n  \n\n  names_ = []\n  average_drop = []\n  average_increase = []\n  average_win = []\n  for item in names:\n    if (item in names_exclude):\n        continue\n    names_.append(item)\n    average_drop.append(resnet_avg_drop[item])\n    average_increase.append(resnet_avg_increase[item])\n    average_win.append(resnet_avg_win[item])\n  \n  data = {'name':names_,'Average_drop':average_drop, 'Average_increase':average_increase,'Average_win':average_win}\n  df = pd.DataFrame(data)\n  df.to_csv(filename)\n  print(filename)\n  print(df)\n\n","metadata":{"id":"YApyv8C-L5Uk","execution":{"iopub.status.busy":"2024-01-07T19:11:40.640787Z","iopub.execute_input":"2024-01-07T19:11:40.641075Z","iopub.status.idle":"2024-01-07T19:11:40.659469Z","shell.execute_reply.started":"2024-01-07T19:11:40.641050Z","shell.execute_reply":"2024-01-07T19:11:40.658469Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# print(vgg_avg_win)\nvgg_win_metrics(names_exclude=[])\n\n#xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n# print('\\n\\n\\n')\n\nresnet_win_metrics(names_exclude=['sccorecam_true','scorecam_true_x'])\n","metadata":{"id":"qad7I2f7tmt_","execution":{"iopub.status.busy":"2024-01-07T19:11:40.660851Z","iopub.execute_input":"2024-01-07T19:11:40.661624Z","iopub.status.idle":"2024-01-07T19:11:40.684424Z","shell.execute_reply.started":"2024-01-07T19:11:40.661590Z","shell.execute_reply":"2024-01-07T19:11:40.683540Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"vgg_metrics_scorecam.csv\n         name  Average_drop  Average_increase  Average_win\n0     gradcam     10.920070              42.0         22.0\n1   gradcampp     13.173542              26.0          2.0\n2    xgradcam     10.189994              40.0         26.0\n3    layercam     12.980695              24.0          0.0\n4    scorecam     11.678690              38.0         12.0\n5  scorecam_x      8.267951              38.0         30.0\n6       augpp     12.158997              30.0          8.0\nresnet_metrics_scorecam.csv\n         name  Average_drop  Average_increase  Average_win\n0     gradcam     10.197578              18.0          6.0\n1   gradcampp     10.247979              18.0          0.0\n2    xgradcam     10.168956              18.0          8.0\n3    layercam     10.223560              18.0          0.0\n4    scorecam      7.845512              24.0         10.0\n5  scorecam_x      4.358351              46.0         66.0\n6       augpp     10.511366              20.0         10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# import torchvision.models as models\n# resnet = models.resnet18(pretrained=True).eval()\n# resnet_model_dict = dict(type='resnet18', arch=resnet, layer_name='layer4',input_size=(224, 224))\n# print(resnet)\nvgg = models.vgg19(pretrained=True).eval()\nprint(vgg)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T19:11:40.687300Z","iopub.execute_input":"2024-01-07T19:11:40.687735Z","iopub.status.idle":"2024-01-07T19:11:42.240522Z","shell.execute_reply.started":"2024-01-07T19:11:40.687709Z","shell.execute_reply":"2024-01-07T19:11:42.239536Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): ReLU(inplace=True)\n    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (26): ReLU(inplace=True)\n    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): ReLU(inplace=True)\n    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (33): ReLU(inplace=True)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): ReLU(inplace=True)\n    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}